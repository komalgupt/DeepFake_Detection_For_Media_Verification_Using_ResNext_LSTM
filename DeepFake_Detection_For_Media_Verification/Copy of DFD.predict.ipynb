{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1g2JPCTB7FOsmJ3kNGvNWAZd1HhwsDlMV","timestamp":1746644839200}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Sw3jwnCMu4W","executionInfo":{"status":"ok","timestamp":1746708018340,"user_tz":-330,"elapsed":13661,"user":{"displayName":"Komal Gupta","userId":"01587743458875510976"}},"outputId":"eb5d6772-39f7-40eb-ccd0-36c55563b454"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["\n","# Imports\n","import torch\n","import torchvision\n","from torch import nn\n","from torchvision import transforms, models\n","from torch.utils.data import Dataset\n","import os\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","# Constants\n","im_size = 112\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","sm = nn.Softmax()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Inverse normalization for image visualization\n","inv_normalize = transforms.Normalize(mean=-1 * np.divide(mean, std), std=np.divide([1, 1, 1], std))\n","\n","# Image conversion\n","def im_convert(tensor):\n","    image = tensor.to(\"cpu\").clone().detach()\n","    image = image.squeeze()\n","    image = inv_normalize(image)\n","    image = image.numpy()\n","    image = image.transpose(1, 2, 0)\n","    image = image.clip(0, 1)\n","    cv2.imwrite('./2.png', image * 255)\n","    return image\n","\n","# Define the Model\n","class Model(nn.Module):\n","    def __init__(self, num_classes, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n","        super(Model, self).__init__()\n","        model = models.resnext50_32x4d(pretrained=True)\n","        self.model = nn.Sequential(*list(model.children())[:-2])\n","        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)\n","        self.relu = nn.LeakyReLU()\n","        self.dp = nn.Dropout(0.4)\n","        self.linear1 = nn.Linear(2048, num_classes)\n","        self.avgpool = nn.AdaptiveAvgPool2d(1)\n","\n","    def forward(self, x):\n","        batch_size, seq_length, c, h, w = x.shape\n","        x = x.view(batch_size * seq_length, c, h, w)\n","        fmap = self.model(x)\n","        x = self.avgpool(fmap)\n","        x = x.view(batch_size, seq_length, 2048)\n","        x_lstm, _ = self.lstm(x, None)\n","        return fmap, self.dp(self.linear1(x_lstm[:, -1, :]))\n","\n","# Download OpenCV DNN Face Detection Files\n","!wget -O deploy.prototxt https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt\n","!wget -O res10_300x300_ssd_iter_140000.caffemodel https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel\n","\n","# Load OpenCV DNN model\n","face_net = cv2.dnn.readNetFromCaffe('deploy.prototxt', 'res10_300x300_ssd_iter_140000.caffemodel')\n","\n","# Face Detection\n","def detect_face_dnn(image, confidence_threshold=0.9):\n","    h, w = image.shape[:2]\n","    blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), (104.0, 177.0, 123.0), False, False)\n","    face_net.setInput(blob)\n","    detections = face_net.forward()\n","    faces = []\n","    for i in range(detections.shape[2]):\n","        confidence = detections[0, 0, i, 2]\n","        if confidence > confidence_threshold:\n","            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","            (x1, y1, x2, y2) = box.astype(\"int\")\n","            faces.append((y1, x2, y2, x1))  # top, right, bottom, left\n","    return faces\n","\n","# Dataset for Unseen Video\n","class validation_dataset(Dataset):\n","    def __init__(self, video_names, sequence_length=60, transform=None):\n","        self.video_names = video_names\n","        self.transform = transform\n","        self.count = sequence_length\n","\n","    def __len__(self):\n","        return len(self.video_names)\n","\n","    def __getitem__(self, idx):\n","        video_path = self.video_names[idx]\n","        frames = []\n","        a = int(100 / self.count)\n","        first_frame = np.random.randint(0, a)\n","\n","        for i, frame in enumerate(self.frame_extract(video_path)):\n","            faces = detect_face_dnn(frame)\n","            if faces:\n","                top, right, bottom, left = faces[0]\n","                try:\n","                    face_crop = frame[top:bottom, left:right]\n","                    face_crop = cv2.resize(face_crop, (112, 112))\n","                    if self.transform:\n","                        face_crop = self.transform(face_crop)\n","                    frames.append(face_crop)\n","                except:\n","                    continue\n","            if len(frames) == self.count:\n","                break\n","\n","        if len(frames) < self.count:\n","            pad_count = self.count - len(frames)\n","            empty = torch.zeros_like(frames[0])\n","            frames.extend([empty] * pad_count)\n","\n","        frames = torch.stack(frames)\n","        return frames.unsqueeze(0)\n","\n","    def frame_extract(self, path):\n","        vidObj = cv2.VideoCapture(path)\n","        success = True\n","        while success:\n","            success, image = vidObj.read()\n","            if success:\n","                yield image\n","\n","# Predict Function\n","def predict(model, img, path='./'):\n","    fmap, logits = model(img.to(device))\n","    weight_softmax = model.linear1.weight.detach().cpu().numpy()\n","    logits = sm(logits)\n","    _, prediction = torch.max(logits, 1)\n","    confidence = logits[:, int(prediction.item())].item() * 100\n","    print('Confidence of prediction:', confidence)\n","\n","    idx = np.argmax(logits.detach().cpu().numpy())\n","    bz, nc, h, w = fmap.shape\n","    out = np.dot(fmap[-1].detach().cpu().numpy().reshape((nc, h * w)).T, weight_softmax[idx, :].T)\n","    predict = out.reshape(h, w)\n","    predict = predict - np.min(predict)\n","    predict_img = predict / np.max(predict)\n","    predict_img = np.uint8(255 * predict_img)\n","    out = cv2.resize(predict_img, (im_size, im_size))\n","    heatmap = cv2.applyColorMap(out, cv2.COLORMAP_JET)\n","    img = im_convert(img[:, -1, :, :, :])\n","    result = heatmap * 0.5 + img * 0.8 * 255\n","    cv2.imwrite('/content/1.png', result)\n","    result1 = heatmap * 0.5 / 255 + img * 0.8\n","    r, g, b = cv2.split(result1)\n","    result1 = cv2.merge((r, g, b))\n","    plt.imshow(result1)\n","    plt.show()\n","    return [int(prediction.item()), confidence]\n","\n","# Define transforms\n","video_transforms = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((im_size, im_size)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std)\n","])\n","\n","# Load the trained model\n","model = Model(num_classes=2).to(device)\n","model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/DFD_Final_year/checkpoint(DFDmodel).pt', map_location=device))\n","model.eval()\n","\n","# Input path to unseen videos\n","path_to_videos = [\"/content/drive/My Drive/fakefaces_data/512.mp4\"]\n","video_dataset = validation_dataset(path_to_videos, sequence_length=20, transform=video_transforms)\n","\n","# Predict on unseen video\n","for i in range(len(path_to_videos)):\n","    print(f\"\\nProcessing video: {path_to_videos[i]}\")\n","    prediction = predict(model, video_dataset[i], './')\n","    print(\"Prediction:\", \"REAL\" if prediction[0] == 1 else \"FAKE\")\n","    print(f\"Confidence: {prediction[1]:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tuPETQlsOfnn","executionInfo":{"status":"error","timestamp":1746708033581,"user_tz":-330,"elapsed":15242,"user":{"displayName":"Komal Gupta","userId":"01587743458875510976"}},"outputId":"f050a8b2-ede5-4192-ae92-c96072959bb4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-05-08 12:40:08--  https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 28104 (27K) [text/plain]\n","Saving to: ‘deploy.prototxt’\n","\n","\rdeploy.prototxt       0%[                    ]       0  --.-KB/s               \rdeploy.prototxt     100%[===================>]  27.45K  --.-KB/s    in 0s      \n","\n","2025-05-08 12:40:08 (92.5 MB/s) - ‘deploy.prototxt’ saved [28104/28104]\n","\n","--2025-05-08 12:40:08--  https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel\n","Resolving github.com (github.com)... 20.205.243.166\n","Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel [following]\n","--2025-05-08 12:40:09--  https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10666211 (10M) [application/octet-stream]\n","Saving to: ‘res10_300x300_ssd_iter_140000.caffemodel’\n","\n","res10_300x300_ssd_i 100%[===================>]  10.17M  --.-KB/s    in 0.03s   \n","\n","2025-05-08 12:40:10 (337 MB/s) - ‘res10_300x300_ssd_iter_140000.caffemodel’ saved [10666211/10666211]\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Processing video: /content/drive/My Drive/fakefaces_data/512.mp4\n"]},{"output_type":"error","ename":"IndexError","evalue":"list index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-e24c372dd719>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_videos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nProcessing video: {path_to_videos[i]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prediction:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"REAL\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"FAKE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Confidence: {prediction[1]:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-e24c372dd719>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mpad_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mempty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpad_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","source":["!pip freeze > requirements.txt\n"],"metadata":{"id":"6DuKu_B-sz8p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download('requirements.txt')\n"],"metadata":{"id":"j2uaEXeFs2Ng"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"jwtOCSc6sx45"}}]}